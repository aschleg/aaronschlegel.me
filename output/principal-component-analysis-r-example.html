<!DOCTYPE html>
<html lang="english">
<head>

        <title>Principal Component Analysis with R Example</title>
        <meta charset="utf-8" />
        <link href="https://aaronschlegel.me/feed/all.xml" type="application/atom+xml" rel="alternate" title="Aaron Schlegel's Notebook of Interesting Things Full Atom Feed" />
        <link href="https://aaronschlegel.me/feed/r.xml" type="application/atom+xml" rel="alternate" title="Aaron Schlegel's Notebook of Interesting Things Categories Atom Feed" />


        <!-- Mobile viewport optimized: j.mp/bplateviewport -->
        <meta name="viewport" content="width=device-width,initial-scale=1, maximum-scale=1">
        <meta name="description" content="Often, it is not helpful or informative to only look at all the variables in a dataset for correlations or covariances. A preferable approach is to derive new variables from the original variables that preserve most of the information given by their variances. Principal component analysis is a widely used and popular statistical method for reducing data with many dimensions (variables) by projecting the data with fewer dimensions using linear combinations of the variables, known as principal components." />
        <meta property="og:site_name" content="Aaron Schlegel's Notebook of Interesting Things" />
        <meta property="og:type" content="article" />
        <meta property="og:title" content="Principal Component Analysis with R Example" />
        <meta property="og:url" content="https://aaronschlegel.me" />
        <meta property="og:description" content="" />
        <meta property="article:published_time" content="2017-01-19 00:00:00-08:00" />
        <meta property="article:modified_time" content="" />
        <meta name="twitter:site" content="@Aaron_Schlegel" />
        <meta name="twitter:creator" content="@Aaron_Schlegel" />
        <meta name="twitter:card" content="Often, it is not helpful or informative to only look at all the variables in a dataset for correlations or covariances. A preferable approach is to derive new variables from the original variables that preserve most of the information given by their variances. Principal component analysis is a widely used and popular statistical method for reducing data with many dimensions (variables) by projecting the data with fewer dimensions using linear combinations of the variables, known as principal components." />
        <meta name="twitter:card" content="The Blog and Notebooks of Aaron Schlegel" />

        <link rel="stylesheet" type="text/css" href="https://aaronschlegel.me/theme/css/styles.min.css" />
        <link rel="canonical" href="https://aaronschlegel.me/principal-component-analysis-r-example.html" />

        <script src="https://aaronschlegel.me/theme/js/libs/modernizr-2.6.2.min.js"></script>

              <script>
                (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

                ga('create', 'UA-48350829-2', 'aaronschlegel.me');
                ga('send', 'pageview');

              </script>


</head>

<body id="index" class="home">
    <div class="container">
        <div class="row">

            <div id="navigation" class="navbar row">
              <a href="#" gumby-trigger="#navigation &gt; ul" class="toggle"><i class="icon-menu"></i></a>

              <ul class="columns-right">
                <li><a href="https://aaronschlegel.me/">Home</a></li>

                <li><a href="https://aaronschlegel.me/pages/projects.html">Projects</a></li>

              </ul>
            </div>

<section id="content" class="body">

   <div class="row">
        <div class="eleven columns">
            <header>
              <h2 class="entry-title">
                <a href="https://aaronschlegel.me/principal-component-analysis-r-example.html" rel="bookmark"
                   title="Permalink to Principal Component Analysis with R Example">Principal Component Analysis with R Example</a></h2>
           
            </header>
            <footer class="post-info">
              <abbr class="published" title="2017-01-19T00:00:00-08:00">
                Thu 19 January 2017
              </abbr>
              <address class="vcard author">By 
                <a class="url fn" href="https://aaronschlegel.me/author/aaron-schlegel.html"> Aaron Schlegel</a>
              </address>
            </footer><!-- /.post-info -->
            <div class="entry-content">
                <p>Often, it is not helpful or informative to only look at all the
variables in a dataset for correlations or covariances. A preferable
approach is to derive new variables from the original variables that
preserve most of the information given by their variances. Principal
component analysis is a widely used and popular statistical method for
reducing data with many dimensions (variables) by projecting the data
with fewer dimensions using linear combinations of the variables, known
as principal components. The new projected variables (principal
components) are uncorrelated with each other and are ordered so that the
first few components retain most of the variation present in the
original variables. Thus, PCA is also useful in situations where the
independent variables are correlated with each other and can be employed
in exploratory data analysis or for making predictive models. Principal
component analysis can also reveal important features of the data such
as outliers and departures from a multinormal distribution.</p>
<h2>Defining Principal Components</h2>
<p>The first step in defining the principal components of <span class="math">\(p\)</span> original
variables is to find a linear function <span class="math">\(a_1'y\)</span>, where <span class="math">\(a_1\)</span> is a vector
of <span class="math">\(p\)</span> constants, for the observation vectors that have maximum
variance. This linear function is defined as:</p>
<div class="math">$$ a_1'y = a_{11}x_1 + a_{12}x_2 + \cdots + a_{1p}x_p = \sum_{j=1}^p a_{1j}x_j $$</div>
<p>Principal component analysis continues to find a linear function <span class="math">\(a_2'y\)</span>
that is uncorrelated with <span class="math">\(a_1'y\)</span> with maximized variance and so on up
to <span class="math">\(k\)</span> principal components.</p>
<h2>Derivation of Principal Components</h2>
<p>The principal components of a dataset are obtained from the sample
covariance matrix <span class="math">\(S\)</span> or the correlation matrix <span class="math">\(R\)</span>. Although principal
components obtained from <span class="math">\(S\)</span> is the original method of principal
component analysis, components from <span class="math">\(R\)</span> may be more interpretable if the
original variables have different units or wide variances (Rencher 2002,
pp. 393). For now, <span class="math">\(S\)</span> will be referred to as <span class="math">\(\Sigma\)</span> (denotes a known
covariance matrix) which will be used in the derivation.</p>
<p>The goal of the derivation is to find <span class="math">\(a'_ky\)</span> that maximizes the
variance of <span class="math">\(a'_ky \Sigma a_k\)</span>. For this, we will consider the first
vector <span class="math">\(a'_1y\)</span> that maximizes <span class="math">\(Var(a'_1y) = a'_1y \Sigma a_1\)</span>. To do
this maximization, we will need a constraint to rein in otherwise
unnecessarily large values of <span class="math">\(a_1\)</span>. The constraint in this example is
the unit length vector <span class="math">\(a_1' a_1 = 1\)</span>. This constraint is employed with
a Lagrange multiplier <span class="math">\(\lambda\)</span> so that the function is maximized at an
equality constraint of <span class="math">\(g(x) = 0\)</span>. Thus the Lagrangian function is
defined as:</p>
<div class="math">$$ a'_1 \Sigma a_1 - \lambda(a_1'y a_1 - 1) $$</div>
<h2>Brief Aside: Lagrange Multipliers</h2>
<p>The Lagrange mulitiplier method is used for finding a maximum or minimum
of a multivariate function with some constraint on the input values. As
we are interested in maximization, the problem can be briefly stated as
'maximize <span class="math">\(f(x)\)</span> subject to <span class="math">\(g(x) = c\)</span>'. In this example,
<span class="math">\(g(x) = a_1'y a_1 = 1\)</span> and <span class="math">\(f(x) = a_1'y \Sigma a_1\)</span>. The Lagrange
multiplier, defined as <span class="math">\(\lambda\)</span> allows the combination of <span class="math">\(f(x)\)</span> and
<span class="math">\(g(x)\)</span> into a new function <span class="math">\(L(x, \lambda)\)</span>, defined as:</p>
<div class="math">$$ L(a_1'y, \lambda) = f(a_1'y) - \lambda(g(a_1'y) - c) $$</div>
<p>The sign of <span class="math">\(\lambda\)</span> can be positive or negative. The new function is
then solved for a stationary point, in this case <span class="math">\(0\)</span>, using partial
derivatives:</p>
<div class="math">$$ \frac{\partial L(a_1'y, \lambda)}{\partial a_1'y} = 0 $$</div>
<p>Returning to principal component analysis, we differentiate
<span class="math">\(L(a_1) = a'_1 \Sigma a_1 - \lambda(a_1'y a_1 - 1)\)</span> with respect to
<span class="math">\(a_1\)</span>:</p>
<div class="math">$$ \frac{\partial L}{\partial a_1} = 2\Sigma a_1 - 2\lambda a_1 = 0 $$</div>
<div class="math">$$ \Sigma a_1 - \lambda a_1 = 0 $$</div>
<p>Expressing the above with an identity matrix, <span class="math">\(I\)</span>:</p>
<div class="math">$$ (\Sigma - \lambda I) a_1 = 0 $$</div>
<p>Which shows <span class="math">\(\lambda\)</span> is an eigenvector of the covariance matrix
<span class="math">\(\Sigma\)</span> and <span class="math">\(a_1\)</span> is the corresponding eigenvector. As stated
previously, we are interested in finding <span class="math">\(a_1'y\)</span> with maximum variance.
Therefore <span class="math">\(\lambda\)</span> must be as large as possible which follows <span class="math">\(a_1\)</span> is
the eigenvector corresponding to the largest eigenvalue of <span class="math">\(\Sigma\)</span>.</p>
<p>The remaining principal components are found in a similar manner and
correspond to the <span class="math">\(k\)</span>th principal component. Thus the second principal
component is <span class="math">\(a_2'y\)</span> and is equivalent to the eigenvector of the second
largest eigenvalue of <span class="math">\(\Sigma\)</span>, and so on.</p>
<h2>Principal Component Analysis</h2>
<p>Twenty engineer apprentices and twenty pilots were given six tests. The
data were obtained from the <a href="ftp://ftp.wiley.com">companion FTP site</a> of
the book Methods of Multivariate Analysis by Alvin Rencher. The tests
measured the following attributes:</p>
<ul>
<li>Intelligence</li>
<li>Form Relations</li>
<li>Dynamometer</li>
<li>Dotting</li>
<li>Sensory Motor Coordination</li>
<li>Perservation</li>
</ul>
<p>Principal component analysis will be performed on the data to transform
the attributes into new variables that will hopefully be more open to
interpretation and allow us to find any irregularities in the data such
as outliers.</p>
<p>Load the data and name the columns. The factors in the <code>Group</code> column
are renamed to their actual grouping names.</p>
<div class="highlight"><pre><span></span><span class="n">pilots</span> <span class="o">&lt;-</span> <span class="nf">read.table</span><span class="p">(</span><span class="s">&#39;PILOTS.DAT&#39;</span><span class="p">,</span> <span class="n">col.names</span> <span class="o">=</span>  <span class="nf">c</span><span class="p">(</span><span class="s">&#39;Group&#39;</span><span class="p">,</span> <span class="s">&#39;Intelligence&#39;</span><span class="p">,</span> <span class="s">&#39;Form Relations&#39;</span><span class="p">,</span>
                                                  <span class="s">&#39;Dynamometer&#39;</span><span class="p">,</span> <span class="s">&#39;Dotting&#39;</span><span class="p">,</span> <span class="s">&#39;Sensory Motor Coordination&#39;</span><span class="p">,</span>
                                                  <span class="s">&#39;Perservation&#39;</span><span class="p">))</span>
<span class="n">pilots</span><span class="o">$</span><span class="n">Group</span> <span class="o">&lt;-</span> <span class="nf">ifelse</span><span class="p">(</span><span class="n">pilots</span><span class="o">$</span><span class="n">Group</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="s">&#39;Apprentice&#39;</span><span class="p">,</span> <span class="s">&#39;Pilot&#39;</span><span class="p">)</span>
</pre></div>


<p>Inspect the first few rows of the data.</p>
<div class="highlight"><pre><span></span><span class="nf">head</span><span class="p">(</span><span class="n">pilots</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">##        Group Intelligence Form.Relations Dynamometer Dotting</span>
<span class="err">## 1 Apprentice          121             22          74     223</span>
<span class="err">## 2 Apprentice          108             30          80     175</span>
<span class="err">## 3 Apprentice          122             49          87     266</span>
<span class="err">## 4 Apprentice           77             37          66     178</span>
<span class="err">## 5 Apprentice          140             35          71     175</span>
<span class="err">## 6 Apprentice          108             37          57     241</span>
<span class="err">##   Sensory.Motor.Coordination Perservation</span>
<span class="err">## 1                         54          254</span>
<span class="err">## 2                         40          300</span>
<span class="err">## 3                         41          223</span>
<span class="err">## 4                         80          209</span>
<span class="err">## 5                         38          261</span>
<span class="err">## 6                         59          245</span>
</pre></div>


<p>The variables appear to be measured in different units which may lead to
the variables with larger variances dominating the principal components
of the covariance matrix <span class="math">\(S\)</span>. We will perform principal component
analysis on the correlation matrix <span class="math">\(R\)</span> later in the example to find a
scaled and more balanced representation of the components.</p>
<p>Find the covariance matrix <span class="math">\(S\)</span> of the data. The grouping column is not
included.</p>
<div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">&lt;-</span> <span class="nf">cov</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">])</span>
<span class="n">S</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">##                            Intelligence Form.Relations Dynamometer</span>
<span class="err">## Intelligence                  528.19487       35.98974    27.97949</span>
<span class="err">## Form.Relations                 35.98974       68.91282   -12.09744</span>
<span class="err">## Dynamometer                    27.97949      -12.09744   145.18974</span>
<span class="err">## Dotting                       104.42821      -81.75128   128.88205</span>
<span class="err">## Sensory.Motor.Coordination    -20.03077      -33.00513   -30.85641</span>
<span class="err">## Perservation                  291.15385      -18.28205    29.38462</span>
<span class="err">##                               Dotting Sensory.Motor.Coordination</span>
<span class="err">## Intelligence                104.42821                  -20.03077</span>
<span class="err">## Form.Relations              -81.75128                  -33.00513</span>
<span class="err">## Dynamometer                 128.88205                  -30.85641</span>
<span class="err">## Dotting                    1366.43013                 -113.58077</span>
<span class="err">## Sensory.Motor.Coordination -113.58077                  264.35641</span>
<span class="err">## Perservation                395.18590                  -79.85897</span>
<span class="err">##                            Perservation</span>
<span class="err">## Intelligence                  291.15385</span>
<span class="err">## Form.Relations                -18.28205</span>
<span class="err">## Dynamometer                    29.38462</span>
<span class="err">## Dotting                       395.18590</span>
<span class="err">## Sensory.Motor.Coordination    -79.85897</span>
<span class="err">## Perservation                 1069.11538</span>
</pre></div>


<p>The total variance is defined as:</p>
<div class="math">$$ \sum^k_{j=1} s_{jj} $$</div>
<p>Which is also equal to the sum of the eigenvalues of <span class="math">\(S\)</span>.</p>
<div class="highlight"><pre><span></span><span class="nf">sum</span><span class="p">(</span><span class="nf">diag</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## [1] 3442.199</span>
</pre></div>


<p>Compute the eigenvalues and corresponding eigenvectors of <span class="math">\(S\)</span>.</p>
<div class="highlight"><pre><span></span><span class="n">s.eigen</span> <span class="o">&lt;-</span> <span class="nf">eigen</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="n">s.eigen</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## eigen() decomposition</span>
<span class="err">## $values</span>
<span class="err">## [1] 1722.0424  878.3578  401.4386  261.0769  128.9051   50.3785</span>
<span class="err">## </span>
<span class="err">## $vectors</span>
<span class="err">##             [,1]        [,2]        [,3]        [,4]        [,5]</span>
<span class="err">## [1,] -0.21165160 -0.38949336  0.88819049  0.03082062 -0.04760343</span>
<span class="err">## [2,]  0.03883125 -0.06379320  0.09571590 -0.19128493 -0.14793191</span>
<span class="err">## [3,] -0.08012946  0.06602004  0.08145863 -0.12854488  0.97505667</span>
<span class="err">## [4,] -0.77552673  0.60795970  0.08071120  0.08125631 -0.10891968</span>
<span class="err">## [5,]  0.09593926 -0.01046493  0.01494473  0.96813856  0.10919120</span>
<span class="err">## [6,] -0.58019734 -0.68566916 -0.43426141  0.04518327  0.03644629</span>
<span class="err">##             [,6]</span>
<span class="err">## [1,]  0.10677164</span>
<span class="err">## [2,] -0.96269790</span>
<span class="err">## [3,] -0.12379748</span>
<span class="err">## [4,] -0.06295166</span>
<span class="err">## [5,] -0.20309559</span>
<span class="err">## [6,] -0.03572141</span>
</pre></div>


<p>The eigenvectors represent the principal components of <span class="math">\(S\)</span>. The
eigenvalues of <span class="math">\(S\)</span> are used to find the proportion of the total variance
explained by the components.</p>
<div class="highlight"><pre><span></span><span class="nf">for </span><span class="p">(</span><span class="n">s</span> <span class="n">in</span> <span class="n">s.eigen</span><span class="o">$</span><span class="n">values</span><span class="p">)</span> <span class="p">{</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">s.eigen</span><span class="o">$</span><span class="n">values</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## [1] 0.5002739</span>
<span class="err">## [1] 0.2551734</span>
<span class="err">## [1] 0.1166227</span>
<span class="err">## [1] 0.07584597</span>
<span class="err">## [1] 0.03744848</span>
<span class="err">## [1] 0.01463556</span>
</pre></div>


<p>The first two principal components account for 75.5% of the total
variance. A scree graph of the eigenvalues can be plotted to visualize
the proportion of variance explained by each subsequential eigenvalue.</p>
<div class="highlight"><pre><span></span><span class="nf">plot</span><span class="p">(</span><span class="n">s.eigen</span><span class="o">$</span><span class="n">values</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#39;Eigenvalue Number&#39;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#39;Eigenvalue Size&#39;</span><span class="p">,</span> <span class="n">main</span> <span class="o">=</span> <span class="s">&#39;Scree Graph&#39;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">s.eigen</span><span class="o">$</span><span class="n">values</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="figure/pca/unnamed-chunk-8-1.png"></p>
<p>The elements of the eigenvectors of <span class="math">\(S\)</span> are the 'coefficients' or
'loadings' of the principal components.</p>
<div class="highlight"><pre><span></span><span class="n">s.eigen</span><span class="o">$</span><span class="n">vectors</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">##             [,1]        [,2]        [,3]        [,4]        [,5]</span>
<span class="err">## [1,] -0.21165160 -0.38949336  0.88819049  0.03082062 -0.04760343</span>
<span class="err">## [2,]  0.03883125 -0.06379320  0.09571590 -0.19128493 -0.14793191</span>
<span class="err">## [3,] -0.08012946  0.06602004  0.08145863 -0.12854488  0.97505667</span>
<span class="err">## [4,] -0.77552673  0.60795970  0.08071120  0.08125631 -0.10891968</span>
<span class="err">## [5,]  0.09593926 -0.01046493  0.01494473  0.96813856  0.10919120</span>
<span class="err">## [6,] -0.58019734 -0.68566916 -0.43426141  0.04518327  0.03644629</span>
<span class="err">##             [,6]</span>
<span class="err">## [1,]  0.10677164</span>
<span class="err">## [2,] -0.96269790</span>
<span class="err">## [3,] -0.12379748</span>
<span class="err">## [4,] -0.06295166</span>
<span class="err">## [5,] -0.20309559</span>
<span class="err">## [6,] -0.03572141</span>
</pre></div>


<p>The first two principal components are thus:</p>
<div class="math">$$ z_1 = a'_1y = -.212y_1 + .039y_2 - 0.080y_3 - 0.776y_4 + 0.096y_5 - 0.580y_6 $$</div>
<div class="math">$$ z_2 = a'_2y = -.389y_1 - .064y_2 + 0.066y_3 + 0.608y_4 - 0.010y_5 - 0.686y_6 $$</div>
<h2>Principal Component Analysis with R</h2>
<p>Computing the principal components in R is straightforward with the
functions <code>prcomp()</code> and <code>princomp()</code>. The difference between the two is
simply the method employed to calculate PCA. According to <code>?prcomp</code>:</p>
<blockquote>
<p>The calculation is done by a singular value decomposition of the
(centered and possibly scaled) data matrix, not by using eigen on the
covariance matrix. This is generally the preferred method for
numerical accuracy.</p>
</blockquote>
<p>From <code>?princomp</code>:</p>
<blockquote>
<p>The calculation is done using eigen on the correlation or covariance
matrix, as determined by cor. This is done for compatibility with the
S-PLUS result.</p>
</blockquote>
<div class="highlight"><pre><span></span><span class="n">pilots.pca</span> <span class="o">&lt;-</span> <span class="nf">prcomp</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">])</span>
<span class="n">pilots.pca</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## Standard deviations (1, .., p=6):</span>
<span class="err">## [1] 41.497499 29.637102 20.035932 16.157875 11.353640  7.097781</span>
<span class="err">## </span>
<span class="err">## Rotation (n x k) = (6 x 6):</span>
<span class="err">##                                    PC1         PC2         PC3         PC4</span>
<span class="err">## Intelligence                0.21165160 -0.38949336  0.88819049 -0.03082062</span>
<span class="err">## Form.Relations             -0.03883125 -0.06379320  0.09571590  0.19128493</span>
<span class="err">## Dynamometer                 0.08012946  0.06602004  0.08145863  0.12854488</span>
<span class="err">## Dotting                     0.77552673  0.60795970  0.08071120 -0.08125631</span>
<span class="err">## Sensory.Motor.Coordination -0.09593926 -0.01046493  0.01494473 -0.96813856</span>
<span class="err">## Perservation                0.58019734 -0.68566916 -0.43426141 -0.04518327</span>
<span class="err">##                                    PC5         PC6</span>
<span class="err">## Intelligence               -0.04760343 -0.10677164</span>
<span class="err">## Form.Relations             -0.14793191  0.96269790</span>
<span class="err">## Dynamometer                 0.97505667  0.12379748</span>
<span class="err">## Dotting                    -0.10891968  0.06295166</span>
<span class="err">## Sensory.Motor.Coordination  0.10919120  0.20309559</span>
<span class="err">## Perservation                0.03644629  0.03572141</span>
</pre></div>


<p>Although we didn't use the preferred method of applying singular value
decomposition, the components reported by the <code>prcomp()</code> are the same as
what was computed earlier save arbitrary scalings of <span class="math">\(-1\)</span> to some of the
eigenvectors.</p>
<p>The summary method of <code>prcomp()</code> also outputs the proportion of variance
explained by the components.</p>
<div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">pilots.pca</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## Importance of components:</span>
<span class="err">##                            PC1     PC2     PC3      PC4      PC5     PC6</span>
<span class="err">## Standard deviation     41.4975 29.6371 20.0359 16.15788 11.35364 7.09778</span>
<span class="err">## Proportion of Variance  0.5003  0.2552  0.1166  0.07585  0.03745 0.01464</span>
<span class="err">## Cumulative Proportion   0.5003  0.7554  0.8721  0.94792  0.98536 1.00000</span>
</pre></div>


<h2>Plotting of Principal Components</h2>
<p>The first two principal components are often plotted as a scatterplot
which may reveal interesting features of the data, such as departures
from normality, outliers or non-linearity. The first two principal
components are evaluated for each observation vector and plotted.</p>
<p>The <a href="https://cran.r-project.org/web/packages/ggfortify/index.html">ggfortify
package</a>
provides a handy method for plotting the first two principal components
with <code>autoplot()</code>.</p>
<div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggfortify</span><span class="p">)</span>
</pre></div>


<p>The <code>autoplot()</code> function also generates a useful data table of the
calculated principal components we which we will use later.</p>
<div class="highlight"><pre><span></span><span class="n">pca.plot</span> <span class="o">&lt;-</span> <span class="nf">autoplot</span><span class="p">(</span><span class="n">pilots.pca</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pilots</span><span class="p">,</span> <span class="n">colour</span> <span class="o">=</span> <span class="s">&#39;Group&#39;</span><span class="p">)</span>
<span class="n">pca.plot</span>
</pre></div>


<p><img alt="" src="figure/pca/unnamed-chunk-13-1.png"></p>
<p>The points of the two groups are clustered for the most part; however,
the three points at the top of the graph may be outliers. The data does
not appear to depart widely from multivariate normality. We will see if
this conclusion changes when the PCs from the correlation matrix <span class="math">\(R\)</span> are
plotted.</p>
<p>To recreate the graph generated by <code>autoplot()</code>, scale the data using
the standard deviations of the principal components multiplied by the
square root of the number of observations. The principal components are
then computed for each observation vector. Note the first eigenvector is
multiplied by a scaling factor of <span class="math">\(-1\)</span> so the signs what was reported by
the <code>prcomp()</code> function.</p>
<div class="highlight"><pre><span></span><span class="n">scaling</span> <span class="o">&lt;-</span> <span class="n">pilots.pca</span><span class="o">$</span><span class="n">sdev</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">]</span> <span class="o">*</span> <span class="nf">sqrt</span><span class="p">(</span><span class="nf">nrow</span><span class="p">(</span><span class="n">pilots</span><span class="p">))</span>

<span class="n">pc1</span> <span class="o">&lt;-</span> <span class="nf">rowSums</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="nf">sweep</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">],</span> <span class="m">2</span> <span class="p">,</span><span class="nf">colMeans</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">])))</span> <span class="o">*</span> <span class="n">s.eigen</span><span class="o">$</span><span class="n">vectors</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span> <span class="o">*</span> <span class="m">-1</span><span class="p">)</span> <span class="o">/</span> <span class="n">scaling</span><span class="p">[</span><span class="m">1</span><span class="p">])</span>
<span class="n">pc2</span> <span class="o">&lt;-</span> <span class="nf">rowSums</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="nf">sweep</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">],</span> <span class="m">2</span><span class="p">,</span> <span class="nf">colMeans</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">])))</span> <span class="o">*</span> <span class="n">s.eigen</span><span class="o">$</span><span class="n">vectors</span><span class="p">[,</span><span class="m">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">scaling</span><span class="p">[</span><span class="m">2</span><span class="p">])</span>
</pre></div>


<p>Collect the PCs in a <code>data.frame</code> and plot using <code>ggplot</code> (loaded when
<code>ggfortify</code> was loaded).</p>
<div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">pc1</span><span class="p">,</span> <span class="n">pc2</span><span class="p">,</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="s">&#39;Apprentice&#39;</span><span class="p">,</span> <span class="m">20</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="s">&#39;Pilot&#39;</span><span class="p">,</span> <span class="m">20</span><span class="p">)))</span>
<span class="nf">colnames</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#39;PC1&#39;</span><span class="p">,</span> <span class="s">&#39;PC2&#39;</span><span class="p">,</span> <span class="s">&#39;Group&#39;</span><span class="p">)</span>

<span class="nf">ggplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">PC1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">PC2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Group</span><span class="p">))</span> <span class="o">+</span> 
  <span class="nf">geom_point</span><span class="p">()</span>
</pre></div>


<p><img alt="" src="figure/pca/unnamed-chunk-15-1.png"></p>
<p>The scaling employed when calculating the PCs can be omitted. To remove
scaling in the <code>autplot()</code> function, set the <code>scaling</code> argument to 0.</p>
<h2>Principal Component Analysis with the Correlation Matrix <span class="math">\(R\)</span></h2>
<p>As mentioned previously, although principal component analysis is
typically performed on the covariance matrix <span class="math">\(S\)</span>, it often makes more
intuitive sense to apply PCA to the correlation matrix. Cases where
using <span class="math">\(R\)</span> may be preferable to <span class="math">\(S\)</span> include data that is measured in
different units or has wide variances. The pilot data analyzed does not
appear to have commensurate units for each variable, and because we have
very little information regarding the tests and the measurements
collected, it might make sense to employ the <span class="math">\(R\)</span> matrix rather than <span class="math">\(S\)</span>.</p>
<p>The correlation matrix is found with the <code>cor()</code> function.</p>
<div class="highlight"><pre><span></span><span class="n">R</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">])</span>
<span class="n">R</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">##                            Intelligence Form.Relations Dynamometer</span>
<span class="err">## Intelligence                 1.00000000     0.18863907  0.10103566</span>
<span class="err">## Form.Relations               0.18863907     1.00000000 -0.12094150</span>
<span class="err">## Dynamometer                  0.10103566    -0.12094150  1.00000000</span>
<span class="err">## Dotting                      0.12292126    -0.26641020  0.28935484</span>
<span class="err">## Sensory.Motor.Coordination  -0.05360504    -0.24453244 -0.15750071</span>
<span class="err">## Perservation                 0.38744776    -0.06735388  0.07458298</span>
<span class="err">##                               Dotting Sensory.Motor.Coordination</span>
<span class="err">## Intelligence                0.1229213                -0.05360504</span>
<span class="err">## Form.Relations             -0.2664102                -0.24453244</span>
<span class="err">## Dynamometer                 0.2893548                -0.15750071</span>
<span class="err">## Dotting                     1.0000000                -0.18898014</span>
<span class="err">## Sensory.Motor.Coordination -0.1889801                 1.00000000</span>
<span class="err">## Perservation                0.3269606                -0.15021611</span>
<span class="err">##                            Perservation</span>
<span class="err">## Intelligence                 0.38744776</span>
<span class="err">## Form.Relations              -0.06735388</span>
<span class="err">## Dynamometer                  0.07458298</span>
<span class="err">## Dotting                      0.32696061</span>
<span class="err">## Sensory.Motor.Coordination  -0.15021611</span>
<span class="err">## Perservation                 1.00000000</span>
</pre></div>


<p>Find the eigenvalues and eigenvectors of the <span class="math">\(R\)</span> matrix.</p>
<div class="highlight"><pre><span></span><span class="n">r.eigen</span> <span class="o">&lt;-</span> <span class="nf">eigen</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
</pre></div>


<p>As with the covariance matrix, we can compute the proportion of total
variance explained by the eigenvalues.</p>
<div class="highlight"><pre><span></span><span class="nf">for </span><span class="p">(</span><span class="n">r</span> <span class="n">in</span> <span class="n">r.eigen</span><span class="o">$</span><span class="n">values</span><span class="p">)</span> <span class="p">{</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">r</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">r.eigen</span><span class="o">$</span><span class="n">values</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## [1] 0.2958546</span>
<span class="err">## [1] 0.225736</span>
<span class="err">## [1] 0.1787751</span>
<span class="err">## [1] 0.1357993</span>
<span class="err">## [1] 0.08843547</span>
<span class="err">## [1] 0.07539955</span>
</pre></div>


<p>What is readily noticeable is the first eigenvalue accounts for 30% of
total variance compared with 50% of the variance of the <span class="math">\(S\)</span> matrix. The
first two components of <span class="math">\(R\)</span> only account for 52% of total variance while
the last two components have little significance. Thus, one may want to
keep the first four components rather than the first two or three with
the <span class="math">\(S\)</span> matrix.</p>
<p>To perform principal component analysis using the correlation matrix
using the <code>prcomp()</code> function, set the <code>scale</code> argument to <code>TRUE</code>.</p>
<div class="highlight"><pre><span></span><span class="n">pilots.pca.scaled</span> <span class="o">&lt;-</span> <span class="nf">prcomp</span><span class="p">(</span><span class="n">pilots</span><span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">7</span><span class="p">],</span> <span class="n">scale</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
<span class="n">pilots.pca.scaled</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## Standard deviations (1, .., p=6):</span>
<span class="err">## [1] 1.3323392 1.1637937 1.0356884 0.9026604 0.7284317 0.6726049</span>
<span class="err">## </span>
<span class="err">## Rotation (n x k) = (6 x 6):</span>
<span class="err">##                                    PC1        PC2        PC3        PC4</span>
<span class="err">## Intelligence                0.40239072 -0.3964661  0.4617841 -0.3928149</span>
<span class="err">## Form.Relations             -0.09715877 -0.7472294 -0.1752970 -0.1315611</span>
<span class="err">## Dynamometer                 0.38541311  0.2181560 -0.4329575 -0.7177525</span>
<span class="err">## Dotting                     0.54333623  0.3144601 -0.1065065  0.2453920</span>
<span class="err">## Sensory.Motor.Coordination -0.31188931  0.3559400  0.6268314 -0.3992852</span>
<span class="err">## Perservation                0.53629229 -0.1062657  0.4053555  0.3058981</span>
<span class="err">##                                   PC5        PC6</span>
<span class="err">## Intelligence               -0.2103062 -0.5187674</span>
<span class="err">## Form.Relations             -0.2801896  0.5528697</span>
<span class="err">## Dynamometer                 0.2585104  0.1855163</span>
<span class="err">## Dotting                    -0.7066663  0.1869825</span>
<span class="err">## Sensory.Motor.Coordination -0.2012981  0.4279773</span>
<span class="err">## Perservation                0.5201339  0.4155385</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">pilots.pca.scaled</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## Importance of components:</span>
<span class="err">##                           PC1    PC2    PC3    PC4     PC5    PC6</span>
<span class="err">## Standard deviation     1.3323 1.1638 1.0357 0.9027 0.72843 0.6726</span>
<span class="err">## Proportion of Variance 0.2959 0.2257 0.1788 0.1358 0.08844 0.0754</span>
<span class="err">## Cumulative Proportion  0.2959 0.5216 0.7004 0.8362 0.92460 1.0000</span>
</pre></div>


<p>Plot the first two PCs of the correlation matrix using the <code>autoplot()</code>
function.</p>
<div class="highlight"><pre><span></span><span class="n">pca.plot.scaled</span> <span class="o">&lt;-</span> <span class="nf">autoplot</span><span class="p">(</span><span class="n">pilots.pca.scaled</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pilots</span><span class="p">,</span> <span class="n">colour</span> <span class="o">=</span> <span class="s">&#39;Group&#39;</span><span class="p">)</span>
<span class="n">pca.plot.scaled</span>
</pre></div>


<p><img alt="" src="figure/pca/unnamed-chunk-20-1.png"></p>
<p>The points remain clustered for the most part; however, there does
appear to be more points outside that may be considered outliers, though
they don't appear to be too far off from the cluster. It is important to
remember the first two PCs of the <span class="math">\(R\)</span> matrix only represent 52% of the
total variance and thus may not be fully representative of the variance
in the dataset.</p>
<h2>Interpreting Principal Components</h2>
<p>Interpretation of principal components is still a heavily researched
topic in statistics, and although the components may be readily
interpreted in most settings, this is not always the case (Joliffe,
2002).</p>
<p>One method of interpretation of the principal components is to calculate
the correlation between the original data and the component. The
<code>autoplot()</code> function also generates a nice data table with the original
variables and the calculated PCs, which we will use here to find the
correlations.</p>
<p>First, compute the correlations between the data and the calculated
components of the covariance matrix <span class="math">\(S\)</span>.</p>
<div class="highlight"><pre><span></span><span class="n">comps</span> <span class="o">&lt;-</span> <span class="n">pca.plot</span><span class="o">$</span><span class="n">data</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="o">:</span><span class="m">13</span><span class="p">)]</span>
<span class="nf">cor</span><span class="p">(</span><span class="n">comps</span><span class="p">[,</span><span class="m">3</span><span class="o">:</span><span class="m">8</span><span class="p">],</span> <span class="n">comps</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span><span class="m">9</span><span class="p">)])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">##                                   PC1         PC2         PC3</span>
<span class="err">## Intelligence                0.3821610 -0.50227170  0.77431660</span>
<span class="err">## Form.Relations             -0.1941124 -0.22775091  0.23101683</span>
<span class="err">## Dynamometer                 0.2759600  0.16238412  0.13544989</span>
<span class="err">## Dotting                     0.8706127  0.48743512  0.04374714</span>
<span class="err">## Sensory.Motor.Coordination -0.2448631 -0.01907555  0.01841632</span>
<span class="err">## Perservation                0.7363518 -0.62149561 -0.26610222</span>
</pre></div>


<p>The PCs can then be interpreted based on which variables they are most
correlated in either a positive or negative direction. The level at
which the correlations are significant is left to the researcher.</p>
<p>The first component is positively correlated with Dotting, Perservation,
Intelligence and Dynamometer. This correlation suggests the five
variables vary together and when one goes down, the others decrease as
well. The component is most correlated with Dotting at <span class="math">\(.087\)</span> and could
be considered as primarily a measure of Dotting.</p>
<p>The second component is most correlated with Perservation and
Intelligence, both in a negative direction. Dotting is correlated with
the second component in a positive direction, which would indicate that
as Perservation and Intelligence decrease, Dotting increases.</p>
<p>The third component is primarily correlated with Intelligence and not
much else. This component could be viewed as a measure of the
intelligence of the individual apprentice or pilot.</p>
<p>It was decided previously that due to lack of information regarding the
variables and their units of measurement, it makes more sense to use the
correlation matrix <span class="math">\(R\)</span> for performing principal component analysis.
Let's see how the interpretation of the principal components changes
when we use the <span class="math">\(R\)</span> matrix.</p>
<div class="highlight"><pre><span></span><span class="n">comps.scaled</span> <span class="o">&lt;-</span> <span class="n">pca.plot.scaled</span><span class="o">$</span><span class="n">data</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="o">:</span><span class="m">13</span><span class="p">)]</span>
<span class="nf">cor</span><span class="p">(</span><span class="n">comps.scaled</span><span class="p">[,</span><span class="m">3</span><span class="o">:</span><span class="m">8</span><span class="p">],</span> <span class="n">comps.scaled</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span><span class="m">9</span><span class="p">)])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">##                                   PC1        PC2        PC3</span>
<span class="err">## Intelligence                0.5361209 -0.4614047  0.4782644</span>
<span class="err">## Form.Relations             -0.1294484 -0.8696209 -0.1815530</span>
<span class="err">## Dynamometer                 0.5135010  0.2538886 -0.4484090</span>
<span class="err">## Dotting                     0.7239081  0.3659667 -0.1103075</span>
<span class="err">## Sensory.Motor.Coordination -0.4155423  0.4142408  0.6492020</span>
<span class="err">## Perservation                0.7145232 -0.1236713  0.4198220</span>
</pre></div>


<p>The most apparent changes between the correlations of the original
variables and the PCs of the <span class="math">\(S\)</span> and <span class="math">\(R\)</span> matrices are in components 2
and 3. The first principal component is still strongly correlated with
the variables Dotting and Perservation, but now the variables
Intelligence and Dynamometer are much more correlated and could indicate
that as the former two variables decrease, the latter two increase.</p>
<p>The second component is now correlated the most with Forming Relations
and not much else, whereas with the <span class="math">\(S\)</span> matrix the component was
correlated more to Perservation and Intelligence. This difference in
variable correlations between the components of the two matrices may
indicate Perservation and Intelligence were unduly dominating the
variances.</p>
<p>The third component is now most correlated with Sensory Motor
Coordination and secondarily Intelligence and Perservation, which
indicates that subjects with high Sensory Motor Coordination test scores
also have higher Intelligence and Perservation scores.</p>
<h2>Summary</h2>
<p>This post ended up being much longer than I had anticipated but I hope
it is a good introduction to the power and benefits of principal
component analysis. The post covered PCA with the covariance and
correlation matrices as well as plotting and interpreting the principal
components. I plan to continue discussing PCA in the future as there are
many more topics and applications related to the dimension reduction
technique.</p>
<h2>References</h2>
<p><a href="https://amzn.to/2H8GIoi">Joliffe, I. T. (2002). Principal Component Analysis (2nd ed.). Springer.</a></p>
<p><a href="https://amzn.to/39gsldt">Rencher, A. C. (2002). Methods of multivariate analysis. New York: J. Wiley.</a></p>
<p><a href="https://onlinecourses.science.psu.edu/stat505/node/54">https://onlinecourses.science.psu.edu/stat505/node/54</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

                <h3 style="margin-top: 2em;">Related Posts</h3>

                    <ul class="blank">
                        <li><a href="https://aaronschlegel.me/singular-value-decomposition-r.html">Singular Value Decomposition and R Example</a></li>
                        <li><a href="https://aaronschlegel.me/calculate-matrix-inverse-2x2-3x3.html">How to Calculate the Inverse Matrix for 22 and 33 Matrices</a></li>
                        <li><a href="https://aaronschlegel.me/quadratic-discriminant-analysis-several-groups.html">Quadratic Discriminant Analysis of Several Groups</a></li>
                        <li><a href="https://aaronschlegel.me/quadratic-discriminant-analysis-two-groups.html">Quadratic Discriminant Analysis of Two Groups</a></li>
                        <li><a href="https://aaronschlegel.me/linear-discriminant-analysis-classification-several-groups.html">Linear Discriminant Analysis for the Classification of Several Groups</a></li>
                    </ul>
            </div><!-- /.entry-content -->



        </div><!-- /.eleven.columns -->

<div class="three columns">

        <h3>Categories</h3>
        <ul class="blank">
                <li><a href="https://aaronschlegel.me/category/analysis.html">Analysis</a></li>
                <li><a href="https://aaronschlegel.me/category/calculus.html">Calculus</a></li>
                <li><a href="https://aaronschlegel.me/category/finance.html">Finance</a></li>
                <li><a href="https://aaronschlegel.me/category/linear-algebra.html">Linear Algebra</a></li>
                <li><a href="https://aaronschlegel.me/category/machine-learning.html">Machine Learning</a></li>
                <li><a href="https://aaronschlegel.me/category/nasapy.html">nasapy</a></li>
                <li><a href="https://aaronschlegel.me/category/petpy.html">petpy</a></li>
                <li><a href="https://aaronschlegel.me/category/poetpy.html">poetpy</a></li>
                <li><a href="https://aaronschlegel.me/category/python.html">Python</a></li>
                <li><a href="https://aaronschlegel.me/category/r.html">R</a></li>
                <li><a href="https://aaronschlegel.me/category/sql.html">SQL</a></li>
                <li><a href="https://aaronschlegel.me/category/statistics.html">Statistics</a></li>
        </ul>


    <h3>Recent Posts</h3>

    <ul class="blank">
            <li>
              <a href="https://aaronschlegel.me/van-der-waerdens-normal-scores-test.html">Van der Waerden's Normal Scores Test</a>
            </li>
            <li>
              <a href="https://aaronschlegel.me/generalized-black-scholes-formula-european-options.html">The Generalized Black-Scholes Formula for European Options</a>
            </li>
            <li>
              <a href="https://aaronschlegel.me/get-all-nasa-astronomy-pictures-day-2019.html">Get All NASA Astronomy Pictures of the Day from 2019</a>
            </li>
            <li>
              <a href="https://aaronschlegel.me/analyzing-next-decade-earth-close-approaching-objects-nasapy.html">Analyzing the Next Decade of Earth Close-Approaching Objects with nasapy</a>
            </li>
            <li>
              <a href="https://aaronschlegel.me/plot-earth-fireball-impacts-nasapy-pandas-folium.html">Plot Earth Fireball Impacts with nasapy, pandas and folium</a>
            </li>
            <li>
              <a href="https://aaronschlegel.me/integration-by-parts.html">Integration by Parts</a>
            </li>
    </ul>

        <nav class="widget">
          <h3>Blogroll</h3>
          <ul class="blank">
            <li>
                <a href="https://www.r-bloggers.com">R-Bloggers</a>
            </li>
          </ul>
        </nav>

</div> </div><!-- /.row -->


</section>



       </div><!-- /.row -->
    </div><!-- /.container -->
       <div class="container.nopad bg">
        <footer id="credits" class="row">
          <div class="seven columns left-center">

                   <address id="about" class="vcard body">
                    Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                    which takes great advantage of <a href="http://python.org">Python</a>.
                    <br />
                    Based on the <a target="_blank" href="http://gumbyframework.com">Gumby Framework</a>
                    </address>
          </div>


          <div class="seven columns">
            <div class="row">
              <ul class="socbtns">

                <li><div class="btn primary"><a href="https://github.com/aschleg" target="_blank">Github</a></div></li>

                <li><div class="btn twitter"><a href="http://www.twitter.com/Aaron_Schlegel" target="_blank">Twitter</a></div></li>


                <li><div class="btn danger"><a href="https://plus.google.com/u/0/102881569650657098667" target="_blank">Google+</a></div></li>

              </ul>
            </div>
          </div>
        </footer>

    </div>


  <script src="https://aaronschlegel.me/theme/js/libs/jquery-1.9.1.min.js"></script>
  <script src="https://aaronschlegel.me/theme/js/libs/gumby.min.js"></script>
  <script src="https://aaronschlegel.me/theme/js/plugins.js"></script>
</body>
</html>